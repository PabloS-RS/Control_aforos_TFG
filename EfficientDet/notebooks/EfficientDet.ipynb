{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOMiOisnk7qxu7kwVyMOUUc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5Zs-lXbR965H"},"source":["# EfficientDet\n"]},{"cell_type":"markdown","metadata":{"id":"wyXmRWuL_KOm"},"source":["Este cuaderno utiliza la API de Detección de Objetos de TensorFlow 2 para entrenar el modelo 'EfficientDet d2 768x768' con un conjunto de datos personalizado y convertirlo al formato TensorFlow Lite.\n","\n"," 'EfficientDet d2 768x768' se refiere a una variante específica del algoritmo EfficientDet que opera en imágenes con una resolución de 768x768 píxeles.\n","\n","Al trabajar a través de este Colab, se puede crear y descargar un modelo TFLite para ejecutarlo en un PC, un teléfono Android, o un dispositivo de borde como el Raspberry Pi."]},{"cell_type":"markdown","metadata":{"id":"47Vu_eyDAuzw"},"source":["### 1. Preparar el conjunto de datos personalizado"]},{"cell_type":"markdown","metadata":{"id":"enuHh8GYAz0K"},"source":["En este cuaderno es necesario de realizar una carga de un conjunto de imágenes etiquetadas en relación con las aglomeraciones de personas desde una vista aérea. Para ello se han seleccionado varios datasets con imágenes ya etiquetadas con las carcaterísticas que se busca y se han unificado en un único conjunto de datos."]},{"cell_type":"markdown","source":["Al tratarse de un dataset bastante grande la mejor opción es cargarlo desde el drive directamente. Al tratarse de un .zip para darle uso es necesario descomprimir dicho conjunto de datos."],"metadata":{"id":"6iX4bFBaoDgI"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"0I8YCLoknWw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir /content/images\n","!unzip -q '.../TFG_DATASET_EFFICIENTDET.zip' -d /content/images"],"metadata":{"id":"g0URb3MQne9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como el dataset que utilizamos está formado por varios subdatasets distintos, en los que se clasifica la clase persona como \"0\", \"h\", \"human\", \"people\", ..., ejecutamos un script -py que modifica los ficheros de anotacion donde se clasifique a una persona por el nombre de clase \"person\". De esta forma, todos los ficheros de anotaciones clasifican el objeto persona por el mismo nombre de clase.\n","\n","Además se eliminan de los ficheros de anotaciones aquellas clasificaciones a otras clases que no sean personas, ya que no nos i nteresa detectar un coche, un pájaro o una biciclea. De esta forma reducimos los ficheros de anotaciones y por tanto el tiempo de entrenamiento."],"metadata":{"id":"EtrBteq2qDaa"}},{"cell_type":"code","source":["!python3 '.../update_xml_files.py'"],"metadata":{"id":"OK5Ya_LDrofw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["'EfficientDet d2 768x768' opera en imágenes con una resolución de 768x768píxeles.\n","\n","En el archivo de configuración del modelo se incluye el siguiente mensaje: keep_aspect_ratio_resizer, que redimensiona la imagen manteniendo las proporciones entre anchura y altura intactas. El problema viene a la hora de transformar el modelo entrenado en uno TFlite ya que TensorFlow no soporta keep_aspect_ratio_resizer.\n","\n","Hay otros mensajes disponibles que redimensionan una imagen pero ninguno otro lo hace manteniendo las proporciones entre anchura y altura intactas. Es por ello que antes de realizar el entrenamiento ejecutamos el siguiente script que redimensiona todas las imagenes del conjunto de datos a 768x768 y modifica su fichero de anotaciones."],"metadata":{"id":"b04yxeqfHPph"}},{"cell_type":"code","source":["!python3 '.../redimensionar768x768.py'"],"metadata":{"id":"p2CBZzwoIkKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpdlJoztysSQ"},"source":["###2. Instalar dependencias de TensorFlow Object Detection"]},{"cell_type":"markdown","metadata":{"id":"GJMCec7nIDUe"},"source":["Primero, instalar la API de Detección de Objetos de TensorFlow en esta instancia de Google Colab. Esto requiere clonar el [repositorio de modelos de TensorFlow](https://github.com/tensorflow/models) y ejecutar algunos comandos de instalación.\n","\n","La última versión de TensorFlow con la que se ha verificado que funciona este Colab es TF v2.8.0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J4j0HU5xz1cg"},"outputs":[],"source":["# Clona desde Github el repositorio de modelos de tensorflow\n","!pip uninstall Cython -y # Desinstalar Cython como solución temporal para el error \"No se encuentra el módulo 'object_detection'\"\n","!git clone --depth 1 https://github.com/tensorflow/models # La opción --depth 1 indica que solo se clonará el historial de confirmaciones más reciente, lo que ahorra tiempo y espacio."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1y_TwbxXz4wv"},"outputs":[],"source":["# Copiar archivos de configuración en la carpeta models/research\n","%%bash\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ICNbvIzcz7Oh"},"outputs":[],"source":["# Modificar el archivo setup.py para instalar el repositorio tf-models-official dirigido a TF v2.8.0\n","import re\n","with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n","    s = f.read()\n","\n","with open('/content/models/research/setup.py', 'w') as f:\n","    # Establecer la ruta fine_tune_checkpoint\n","    s = re.sub('tf-models-official>=2.5.1',\n","               'tf-models-official==2.8.0', s)\n","    f.write(s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBCXSQQhz-MT"},"outputs":[],"source":["# Instalar la API de Detección de Objetos (NOTE: Esta celda tarda 10 mins. aprox. en ejecutarse)\n","\n","# Necesidad de hacer un cambio temporal con PyYAML porque Colab no puede instalar PyYAML v5.4.1\n","!pip install pyyaml==5.3\n","!pip install /content/models/research/\n","\n","# Necesidad de retroceder a TF v2.8.0 debido a un error de compatibilidad de Colab con TF v2.10 (a partir del 10/03/22)\n","!pip install tensorflow==2.8.0\n","\n","# Instalar la version 11.0 de CUDA (para mantener compatibilidad con TF v2.8.0)\n","!pip install tensorflow_io==0.23.1\n","!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n","!mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n","!wget http://developer.download.nvidia.com/compute/cuda/11.0.2/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","!dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.2-450.51.05-1_amd64.deb\n","!apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\n","!apt-get update && sudo apt-get install cuda-toolkit-11-0\n","!export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64:$LD_LIBRARY_PATH"]},{"cell_type":"markdown","metadata":{"id":"L-DzLAKj2sSq"},"source":["### 3. Preparar los datos de entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"4xPZe8IkQ2yx"},"source":["Hay que crear un mapa de etiquetas para el detector y convertir las imágenes en un formato de archivo de datos llamado TFRecords, que son utilizados por TensorFlow para el entrenamiento. Utilizar scripts de Python para convertir automáticamente los datos al formato TFRecord. Antes de ejecutarlos, definir un mapa de etiquetas para las clases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0v0wd7MTV-J"},"outputs":[],"source":["# Crear un archivo  \"labelmap.txt\" con una lista de las clases que el modelo de detección de objetos detectará\n","%%bash\n","cat <<EOF >> /content/labelmap.txt\n","person\n","EOF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hA65Gq-STo8m"},"outputs":[],"source":["# Descargar y ejecutar los scripts de conversión de datos: create CSV data files and TFRecord files\n","!python3 '/content/gdrive/MyDrive/tfg/EfficientDet/create_csv.py'\n","! wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/create_tfrecord.py\n","!python3 create_tfrecord.py --csv_input=images/train_labels.csv --labelmap=labelmap.txt --image_dir=images/train --output_path=train.tfrecord\n","!python3 create_tfrecord.py --csv_input=images/valid_labels.csv --labelmap=labelmap.txt --image_dir=images/valid --output_path=valid.tfrecord"]},{"cell_type":"markdown","metadata":{"id":"1Ys7M-YDVd0R"},"source":["Almacenaremos las ubicaciones de los archivos TFRecord y Labelmap como variables para poder hacer referencia a ellos más adelante en esta sesión de Colab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BL1jfOZU9qG0"},"outputs":[],"source":["train_record_fname = '/content/train.tfrecord'\n","val_record_fname = '/content/valid.tfrecord'\n","label_map_pbtxt_fname = '/content/labelmap.pbtxt'"]},{"cell_type":"markdown","metadata":{"id":"qdvZfdgW5IFO"},"source":["### 4. Configurar el entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"Ldv-gyVKd3lY"},"source":["En esta sección, se configura el modelo y la configuración de entrenamiento. Especificar qué modelo preentrenado de TensorFlow queremos utilizar desde el Modelo Zoo de Detección de Objetos de TensorFlow 2. Cada modelo también viene con un archivo de configuración que apunta a las ubicaciones de los archivos, establece parámetros de entrenamiento (como la tasa de aprendizaje y el número total de pasos de entrenamiento) y más. Se modificará el archivo de configuración para el trabajo de entrenamiento personalizado.\n","\n","La primera sección de código enumera algunos modelos disponibles en el Modelo Zoo de TF2 y define algunos nombres de archivo que se utilizarán más adelante para descargar el modelo y el archivo de configuración. Esto facilita la gestión del modelo que se utilice y agregar otros modelos a la lista más adelante.\n","\n","Establece la variable \"chosen_model\" para que coincida con el nombre del modelo con el que se desea entrenar.\n","\n","https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"]},{"cell_type":"code","source":["# A partir de Efficientdet d3+ el entrenamiento es interrumpido por falta de memoria\n","model_name = 'efficientdet_d2_coco17_tpu-32'\n","pretrained_checkpoint = 'efficientdet_d2_coco17_tpu-32.tar.gz'\n","base_pipeline_file = 'ssd_efficientdet_d2_768x768_coco17_tpu-8.config'"],"metadata":{"id":"l8lNvx58nXX3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsAwXexN6WQN"},"outputs":[],"source":["# Crear la carpeta \"mymodel\" para guardar los pesos previamente entrenados y archivos de configuración del modelo\n","%mkdir /content/models/mymodel/\n","%cd /content/models/mymodel/\n","\n","# Descargar los pesos del modelo previamente entrenados\n","import tarfile\n","download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n","!wget {download_tar}\n","tar = tarfile.open(pretrained_checkpoint)\n","tar.extractall()\n","tar.close()\n","\n","# Descargar el archivo de configuración del modelo\n","download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n","!wget {download_config}"]},{"cell_type":"markdown","metadata":{"id":"r56v349PgLiN"},"source":["Ahora que se ha descargado el modelo elegido y archivo de configuración, se necesita modificar el archivo de configuración con algunos parámetros de entrenamiento de alto nivel. Las siguientes variables se utilizan para controlar los pasos de entrenamiento:\n","\n","* num_steps: La cantidad total de pasos a utilizar para entrenar el modelo. Usar más pasos si las métricas de pérdida siguen disminuyendo cuando finaliza el entrenamiento. Cuantos más pasos, más tiempo llevará el entrenamiento. El entrenamiento también puede detenerse prematuramente si la pérdida se estabiliza antes de alcanzar el número especificado de pasos.\n","\n","* batch_size: El número de imágenes a utilizar por paso de entrenamiento. Un tamaño de lote más grande permite que un modelo se entrene en menos pasos, pero el tamaño está limitado por la memoria de la GPU disponible para el entrenamiento. Con las GPU utilizadas en las instancias de Colab, 16 es un buen número para modelos SSD y 4 es bueno para modelos EfficientDet por falta de memoria.\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["num_steps = 48325\n","batch_size = 4"],"metadata":{"id":"5I7AzNb49oNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3435,"status":"ok","timestamp":1715035241127,"user":{"displayName":"Pablo Rodilla","userId":"05519374305017857680"},"user_tz":-120},"id":"A2R5lxls68Gu","outputId":"be423f98-2962-426c-d219-a99a9ef985fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total classes: 1\n"]}],"source":["# Establecer las ubicaciones de los archivos y obtener la cantidad de clases para el archivo de configuración\n","pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n","fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n","\n","def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())\n","num_classes = get_num_classes(label_map_pbtxt_fname)\n","print('Total classes:', num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBuBwTlN_gZO"},"outputs":[],"source":["# (Opcional) Mostrar el contenido del archivo de configuración personalizado\n","import re\n","\n","%cd /content/models/mymodel\n","print('writing custom configuration file')\n","\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open('pipeline_file.config', 'w') as f:\n","\n","    # Set fine_tune_checkpoint path\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","\n","    # Set tfrecord files for train and test datasets\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n","\n","    # Set label_map_path\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n","    s = re.sub(\n","        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n","\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","\n","    s = re.sub(r'image_resizer\\s*{\\s*(.*?)\\s*}',\n","           r'image_resizer{\\nfixed_shaped_resizer{\\nwidth:768\\n weight:768\\n}\\n}', s, flags=re.DOTALL)\n","\n","    s = re.sub('max_detections_per_class: [0-9]+',\n","               'max_detections_per_class: {}'.format(2000), s)\n","\n","    s = re.sub('max_total_detections: [0-9]+',\n","               'max_total_detections: {}'.format(2000), s)\n","\n","    s = re.sub('max_number_of_boxes: [0-9]+',\n","               'max_number_of_boxes: {}'.format(2000), s)\n","\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","\n","    s = re.sub('total_steps: [0-9]+',\n","               'total_steps: {}'.format(num_steps), s)\n","\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    f.write(s)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLzQ1PA0__DS"},"outputs":[],"source":["# (Optional) Display the custom configuration file's contents\n","!cat /content/models/mymodel/pipeline_file.config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3SGIIqqYAXhM"},"outputs":[],"source":["# Establecer la ruta al archivo de configuración personalizado y el directorio para almacenar los puntos de control de entrenamiento\n","pipeline_file = '/content/models/mymodel/pipeline_file.config'\n","model_dir = '/content/training/'"]},{"cell_type":"markdown","metadata":{"id":"e3lhO-swDt65"},"source":["## 5. Entrenar el modelo TFLite personalizado para la detección de objetos"]},{"cell_type":"markdown","source":["¡Estamos listos para entrenar el modelo de detección de objetos! Antes de comenzar a entrenar, carguemos una sesión de TensorBoard para monitorear el progreso del entrenamiento."],"metadata":{"id":"XkWJUYVt8YZj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-mGZyfH-DwAg"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir '/content/content/training/train'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulq5C1LVEAOU"},"outputs":[],"source":["# Ejecutar entrenamiento!\n","!python /content/models/research/object_detection/model_main_tf2.py \\\n","    --pipeline_config_path={pipeline_file} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --sample_1_of_n_eval_examples=1"]},{"cell_type":"markdown","source":["## 6.Convertir el modelo a TensorFlow Lite"],"metadata":{"id":"kPg8oMnQDYKl"}},{"cell_type":"markdown","source":["Nuestro modelo está entrenado y listo para ser utilizado para detectar objetos. Primero, necesitamos exportar el gráfico del modelo (un archivo que contiene información sobre la arquitectura y los pesos) a un formato compatible con TensorFlow Lite. Haremos esto usando el script export_tflite_graph_tf2.py.\n","\n","**Importante: antes de ejecutar la siguiente celda es necesario cambiar la config del archivo export_tflite_graph_tf2.py, en concreto el número máximo de detcciones que es capaz de realiza el modelo**"],"metadata":{"id":"-8ClLvgl83Bh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Lr26JxZjRX0"},"outputs":[],"source":["# Crear un directorio para almacenar el modelo TFLite entrenado\n","!mkdir /content/custom_model_lite\n","output_directory = '/content/custom_model_lite'\n","\n","# Ruta al directorio de entrenamiento\n","last_model_path = '/content/training'\n","\n","!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n","    --trained_checkpoint_dir {last_model_path} \\\n","    --output_directory {output_directory} \\\n","    --pipeline_config_path {pipeline_file}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6siEPbzOoBdQ"},"outputs":[],"source":["# Convertir el archivo exportado en un archivo de modelo TFLite\n","import tensorflow as tf\n","\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n","tflite_model = converter.convert()\n","\n","with open('/content/custom_model_lite/detect.tflite', 'wb') as f:\n","  f.write(tflite_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDtaLd-uyrxa"},"outputs":[],"source":["!zip -r /content/training.zip /content/training\n","!zip -r /content/custom_model_lite.zip /content/custom_model_lite\n"]},{"cell_type":"code","source":["!cp  /content/custom_model_lite.zip ...\n","!cp  /content/training.zip ..."],"metadata":{"id":"cmDLAJ_pndU0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7.Calcular mAP"],"metadata":{"id":"RDQrtQhvC3oG"}},{"cell_type":"code","source":[" # Script to run custom TFLite model on test images to detect objects\n","# Source: https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/blob/master/TFLite_detection_image.py\n","\n","# Import packages\n","import os\n","import cv2\n","import numpy as np\n","import sys\n","import glob\n","import random\n","import importlib.util\n","from tensorflow.lite.python.interpreter import Interpreter\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import xml.etree.ElementTree as ET\n","\n","%matplotlib inline\n","\n","def count_person_labels(xml_file):\n","  tree = ET.parse(xml_file)\n","  root = tree.getroot()\n","\n","  person_count = 0\n","  for obj in root.findall('object'):\n","      name = obj.find('name').text\n","      if name == 'person':\n","          person_count += 1\n","\n","  return person_count\n","\n","### Define function for inferencing with TFLite model and displaying results\n","\n","def tflite_detect_images(modelpath, folderpath, lblpath, min_conf=0.01, num_test_images=1, savepath='/content/results', txt_only=False):\n","\n","  print(modelpath)\n","  print(folderpath)\n","  print(lblpath)\n","  print(min_conf)\n","  print(num_test_images)\n","\n","  # Grab filenames of all images in test folder\n","  images = glob.glob(folderpath + '/*.jpg') + glob.glob(folderpath + '/*.JPG') + glob.glob(folderpath + '/*.png') + glob.glob(folderpath + '/*.bmp')\n","\n","  # Load the label map into memory\n","  with open(lblpath, 'r') as f:\n","      labels = [line.strip() for line in f.readlines()]\n","  print('Longitud labels: ' + str(len(labels)))\n","  # Load the Tensorflow Lite model into memory\n","  interpreter = Interpreter(model_path=modelpath)\n","  interpreter.allocate_tensors()\n","\n","  # Get model details\n","  input_details = interpreter.get_input_details()\n","  output_details = interpreter.get_output_details()\n","  print(output_details)\n","  height = input_details[0]['shape'][1]\n","  width = input_details[0]['shape'][2]\n","\n","  float_input = (input_details[0]['dtype'] == np.float32)\n","\n","  input_mean = 127.5\n","  input_std = 127.5\n","\n","  # Randomly select test images\n","  images_to_test = random.sample(images, num_test_images)\n","\n","  # Loop over every image and perform detection\n","  for image_path in images_to_test:\n","      print('Imagen: ' + image_path)\n","      # Obtener el nombre del archivo XML correspondiente\n","      xml_file = os.path.join(folderpath, os.path.splitext(image_path)[0] + '.xml')\n","      aforo_real = count_person_labels(xml_file)\n","      print(xml_file)\n","\n","      # Load image and resize to expected shape [1xHxWx3]\n","      image = cv2.imread(image_path)\n","      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","      imH, imW, _ = image.shape\n","      image_resized = cv2.resize(image_rgb, (width, height))\n","      input_data = np.expand_dims(image_resized, axis=0)\n","\n","      # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n","      if float_input:\n","          input_data = (np.float32(input_data) - input_mean) / input_std\n","\n","      # Perform the actual detection by running the model with the image as input\n","      interpreter.set_tensor(input_details[0]['index'],input_data)\n","      interpreter.invoke()\n","\n","      # Retrieve detection results\n","      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Bounding box coordinates of detected objects\n","      classes = interpreter.get_tensor(output_details[3]['index'])[0] # Class index of detected objects\n","      scores = interpreter.get_tensor(output_details[0]['index'])[0] # Confidence of detected objects\n","\n","      detections = []\n","\n","      print(aforo_real)\n","      aforo = 0\n","\n","      # Loop over all detections and draw detection box if confidence is above minimum threshold\n","      for i in range(len(scores)):\n","          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n","              aforo += 1\n","              # Get bounding box coordinates and draw box\n","              # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\n","              ymin = int(max(1,(boxes[i][0] * imH)))\n","              xmin = int(max(1,(boxes[i][1] * imW)))\n","              ymax = int(min(imH,(boxes[i][2] * imH)))\n","              xmax = int(min(imW,(boxes[i][3] * imW)))\n","\n","              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n","\n","              # Draw label\n","              #object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n","              label = '%s: %d%%' % ('person', int(scores[i]*100)) # Example: 'person: 72%'\n","              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n","              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n","              #cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n","              #cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n","\n","              detections.append(['person', scores[i], xmin, ymin, xmax, ymax])\n","\n","      print(aforo)\n","\n","      # All the results have been drawn on the image, now display the image\n","      if txt_only == False: # \"text_only\" controls whether we want to display the image results or just save them in .txt files\n","        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","        plt.figure(figsize=(12,16))\n","        plt.imshow(image)\n","        plt.show()\n","\n","      # Save detection results in .txt files (for calculating mAP)\n","      elif txt_only == True:\n","\n","        # Get filenames and paths\n","        image_fn = os.path.basename(image_path)\n","        base_fn, ext = os.path.splitext(image_fn)\n","        txt_result_fn = base_fn +'.txt'\n","        txt_savepath = os.path.join(savepath, txt_result_fn)\n","\n","        # Write results to text file\n","        # (Using format defined by https://github.com/Cartucho/mAP, which will make it easy to calculate mAP)\n","        with open(txt_savepath,'w') as f:\n","            for detection in detections:\n","                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n","\n","  return"],"metadata":{"id":"iwquLV6H3jkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%bash\n","git clone https://github.com/Cartucho/mAP /content/mAP\n","cd /content/mAP\n","rm input/detection-results/*\n","rm input/ground-truth/*\n","rm input/images-optional/*\n","wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"],"metadata":{"id":"klrhoDiEootg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp /content/images/test/* /content/mAP/input/images-optional # Copy images and xml files\n","!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Move xml files to the appropriate folder"],"metadata":{"id":"QzFyydktovQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python /content/mAP/scripts/extra/convert_gt_xml.py"],"metadata":{"id":"xiZMzN-5oyPE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up variables for running inference, this time to get detection results saved as .txt files\n","PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n","PATH_TO_MODEL='/content/detect.tflite'   # Path to .tflite model file\n","PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n","PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n","min_conf_threshold=0.4   # Confidence threshold\n","\n","# Use all the images in the test folder\n","image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n","images_to_test = 173 # If there are more than 500 images in the folder, just use 500\n","\n","# Tell function to just save results and not display images\n","txt_only = True\n","\n","# Run inferencing function!\n","print('Starting inference on %d images...' % images_to_test)\n","tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n","print('Finished inferencing!')"],"metadata":{"id":"3CS2HFZ_o2uJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/mAP\n","!python calculate_map_cartucho.py --labels=/content/labelmap.txt"],"metadata":{"id":"fwBmoq8E_DO3"},"execution_count":null,"outputs":[]}]}